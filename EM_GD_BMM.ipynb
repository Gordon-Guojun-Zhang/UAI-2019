{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing EM with GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we set up by choosing the number of clusters $m$, the number of features $n$, and generate samples by use of a BMM with random parameters. The goal is to recover this setting. The sample set is $S$, which is a dictionary containing binary strings $x$. Notice that replicates are gathered together to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of x:  [0.705  0.947  0.488  0.1037 0.5583]\n"
     ]
    }
   ],
   "source": [
    "import nbm\n",
    "import numpy as np\n",
    "m = 2  # number of classes\n",
    "n = 5  # number of features\n",
    "samples = 100000 # sample size\n",
    "# generate the parameters\n",
    "[gtheta, gPhi] = nbm.randInt(m, n)\n",
    "# P_D: the distribution that generates S\n",
    "total = np.zeros(n)\n",
    "# generate samples\n",
    "S = dict()\n",
    "for _ in range(samples):\n",
    "    t = nbm.bitsToDec(nbm.sampling(n, gPhi, gtheta))\n",
    "    x = nbm.decToBits(t, n)\n",
    "    total += x\n",
    "    if t not in S:\n",
    "        S[t] = 0\n",
    "    S[t] += 1\n",
    "avg = np.divide(total, samples)\n",
    "print(\"average of x: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The true average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7029, 0.9459, 0.4863, 0.102 , 0.5571])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtheta[0] * gPhi[0,:] + gtheta[1] * gPhi[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We run EM with unbalanced initialization. One can see that the mixing coefficient of the first component $\\theta_1$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EM: ----------------------------\n",
      "final theta:  [0.5293 0.4707]\n",
      "final Phi:  [[0.4877 0.9034 0.2624 0.0063 0.5751]\n",
      " [0.9495 0.996  0.7418 0.2132 0.5394]]\n",
      "model mean:  [0.705  0.947  0.488  0.1037 0.5583]\n"
     ]
    }
   ],
   "source": [
    "# initialize params\n",
    "print(\"-------------------EM: ----------------------------\")\n",
    "theta = np.array([0.01, 0.99])\n",
    "Phi = np.random.rand(m, n)\n",
    "[theta1, Phi1] = nbm.em(n, m, samples, S, theta, Phi)\n",
    "print(\"final theta: \", theta1)\n",
    "print(\"final Phi: \", Phi1)\n",
    "avg_model = np.matmul(np.transpose(theta1), Phi1)\n",
    "print(\"model mean: \", avg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the gradients at the point EM converges to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradth:  [-1. -1.]\n",
      "gradPhi:  [[ 0.      0.      0.      0.0001 -0.    ]\n",
      " [ 0.      0.      0.      0.     -0.    ]]\n"
     ]
    }
   ],
   "source": [
    "[gradth, gradPhi] = nbm.gradient(n, theta1, Phi1, samples, S)\n",
    "print(\"gradth: \", gradth)\n",
    "print(\"gradPhi: \", gradPhi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the effective number and the effective mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnum:  [0.564 0.436]\n",
      "effmean:  [[0.6033 0.9153 0.4245 0.0001 0.6267]\n",
      " [0.8366 0.9879 0.5702 0.2377 0.4698]]\n"
     ]
    }
   ],
   "source": [
    "effnum = np.zeros(m)\n",
    "for c in range(m):\n",
    "    effnum[c] = nbm.eff_num(n, theta, Phi, samples, S, c)\n",
    "print(\"effnum: \", effnum / samples)\n",
    "effmean = np.zeros((m, n))\n",
    "for c in range(m):\n",
    "    effmean[c, :] = nbm.eff_mean(n, theta, Phi, samples, S, c)\n",
    "print(\"effmean: \", effmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result for GD with random initialization of parameters. The algorithm can converge to $1$-cluster points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------GD: ----------------------------\n",
      "GD converges at  180 steps\n",
      "final ell:  2.526094022996858\n",
      "final theta:  [1. 0.]\n",
      "final Phi:  [[0.705  0.947  0.488  0.1037 0.5583]\n",
      " [0.7926 0.4691 0.2759 0.6606 0.4687]]\n",
      "GD converges at  183 steps\n",
      "final ell:  2.52609402299685\n",
      "final theta:  [0. 1.]\n",
      "final Phi:  [[0.3923 0.4229 0.6484 0.5001 0.7605]\n",
      " [0.705  0.947  0.488  0.1037 0.5583]]\n",
      "GD converges at  4079 steps\n",
      "final ell:  2.512558428601922\n",
      "final theta:  [0.9261 0.0739]\n",
      "final Phi:  [[0.6815 0.9427 0.4472 0.084  0.523 ]\n",
      " [1.     1.     1.     0.3505 1.    ]]\n",
      "GD converges at  5105 steps\n",
      "final ell:  2.512558428602112\n",
      "final theta:  [0.9261 0.0739]\n",
      "final Phi:  [[0.6815 0.9427 0.4472 0.084  0.523 ]\n",
      " [1.     1.     1.     0.3505 1.    ]]\n",
      "GD converges at  2272 steps\n",
      "final ell:  2.465732418434597\n",
      "final theta:  [0.562 0.438]\n",
      "final Phi:  [[0.9134 0.9899 0.6924 0.1844 0.5089]\n",
      " [0.4376 0.8919 0.2257 0.     0.6842]]\n",
      "GD converges at  1882 steps\n",
      "final ell:  2.465732531463657\n",
      "final theta:  [0.562 0.438]\n",
      "final Phi:  [[0.9134 0.9899 0.6924 0.1844 0.5089]\n",
      " [0.4376 0.8919 0.2257 0.     0.6842]]\n",
      "GD converges at  4530 steps\n",
      "final ell:  2.4579614865139368\n",
      "final theta:  [0.5291 0.4709]\n",
      "final Phi:  [[0.4876 0.9033 0.2623 0.0062 0.5751]\n",
      " [0.9494 0.996  0.7417 0.2131 0.5394]]\n",
      "GD converges at  314 steps\n",
      "final ell:  2.526094022996948\n",
      "final theta:  [1. 0.]\n",
      "final Phi:  [[0.705  0.947  0.488  0.1037 0.5583]\n",
      " [0.551  0.8011 0.6875 0.3026 0.7567]]\n",
      "GD converges at  4435 steps\n",
      "final ell:  2.512558428602008\n",
      "final theta:  [0.0739 0.9261]\n",
      "final Phi:  [[1.     1.     1.     0.3505 1.    ]\n",
      " [0.6815 0.9427 0.4472 0.084  0.523 ]]\n",
      "GD converges at  4415 steps\n",
      "final ell:  2.5105795980736123\n",
      "final theta:  [0.9286 0.0714]\n",
      "final Phi:  [[0.6823 0.9429 0.4486 0.0861 0.6012]\n",
      " [1.     1.     1.     0.3315 0.    ]]\n",
      "model mean:  [0.705  0.947  0.488  0.1037 0.5583]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------GD: ----------------------------\")\n",
    "for _ in range(10):\n",
    "    [theta, Phi] = nbm.randInt(m, n)\n",
    "    theta=np.ones(m)/m\n",
    "    [ell, theta, Phi, iterId] = nbm.gradientDescent(n, samples, S, theta, Phi, numIterations=20000, alpha = 0.02, tolerance = 1e-7)\n",
    "    print(\"final ell: \", ell)\n",
    "    print(\"final theta: \", theta)\n",
    "    print(\"final Phi: \", Phi)\n",
    "    [gradth, gradPhi] = nbm.gradient(n, theta, Phi, samples, S)\n",
    "#    print(\"gradth: \", gradth)\n",
    "#    print(\"gradPhi: \", gradPhi)\n",
    "avg_model = np.matmul(np.transpose(theta), Phi)\n",
    "print(\"model mean: \", avg_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
